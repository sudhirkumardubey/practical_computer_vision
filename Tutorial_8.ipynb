{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Image Stitching\n",
    "\n",
    "• Video Stabilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect and match features using knnMatch\n",
    "\n",
    "def getKnnMatches(img_query, img_train, ratio, direction='right', showMatches=True):\n",
    "\n",
    "    # detect keypoints and compute descriptors using ORB\n",
    "    orb = cv2.ORB_create(nfeatures=2000)\n",
    "    kp_query, dsc_query = orb.detectAndCompute(img_query, None) # left image\n",
    "    kp_train, dsc_train = orb.detectAndCompute(img_train, None) # right image\n",
    "\n",
    "    # get matches on the two images\n",
    "    bf = cv2.BFMatcher_create()\n",
    "    if direction == 'right':\n",
    "        matches = bf.knnMatch(dsc_query, dsc_train, k=2)\n",
    "    if direction == 'left':\n",
    "        matches = bf.knnMatch(dsc_train, dsc_query, k=2)\n",
    "    \n",
    "    # appy ratio test\n",
    "    good = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio*n.distance:\n",
    "            good.append([m])\n",
    "\n",
    "    if showMatches:\n",
    "        # draw matches\n",
    "        if direction == 'right':\n",
    "            img_matches = cv2.drawMatchesKnn(img_query, kp_query, img_train, kp_train, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)    \n",
    "\n",
    "        if direction == 'left':\n",
    "            img_matches = cv2.drawMatchesKnn(img_train, kp_train, img_query, kp_query, good, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)    \n",
    "        cv2.imshow(\"matches\", img_matches)\n",
    "        cv2.waitKey()\n",
    "    \n",
    "    if direction == 'right':\n",
    "        return kp_query, kp_train, good\n",
    "    \n",
    "    if direction == 'left':\n",
    "        return kp_train, kp_query, good\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch left image to right image (right is anchored image)\n",
    "def stitch_left(img_left, img_right, ratio, showMatches=True):\n",
    "\n",
    "    cv2.imshow(\"left image\", img_left)\n",
    "    cv2.imshow(\"right image\", img_right)\n",
    "    cv2.waitKey()\n",
    "\n",
    "    # get matches and keypoints of query image (right) in train image (left)\n",
    "    kp_right, kp_left, good = getKnnMatches(img_right, img_left, ratio, 'left')\n",
    "\n",
    "\n",
    "    # we need at least four matches to find homography between the images\n",
    "    if len(good) > 4:\n",
    "        \n",
    "        # extract locations (keypoints) of good matches\n",
    "        p_left, p_right = [], []\n",
    "        p_right = np.float32([kp_right[m[0].queryIdx].pt for m in good])\n",
    "        p_left = np.float32([kp_left[m[0].trainIdx].pt for m in good])\n",
    "        p_right = np.asarray(p_right)\n",
    "        p_left = np.asarray(p_left)\n",
    "\n",
    "        # find homography using RANSAC (left image is src plane, right is target plane)\n",
    "        H, status = cv2.findHomography(p_left, p_right, cv2.RANSAC)\n",
    "        H_inv = np.linalg.inv(H)\n",
    "        print(\"Inverse Homography:\\n\", H_inv)\n",
    "\n",
    "        # calculate positions of warped corners of left image by matrix-vector product\n",
    "        #  p_00 ------------ p_c0\n",
    "        #   |                 |\n",
    "        #  p_0r ------------ p_cr\n",
    "        h = img_left.shape[0]\n",
    "        w = img_left.shape[1]\n",
    "        p_00 = np.dot(H_inv, np.array([0,0,1])) \n",
    "        p_00 = p_00/p_00[-1] # homogenous to cartesian coordinates x=x/w, y=y/w\n",
    "        p_0r = np.dot(H_inv, np.array([0, h, 1])) \n",
    "        p_0r = p_0r/p_0r[-1] \n",
    "        p_c0 = np.dot(H_inv, np.array([w, 0, 1])) \n",
    "        p_c0 = p_c0/p_c0[-1] \n",
    "        p_cr = np.dot(H_inv, np.array([w, h, 1])) \n",
    "        p_cr = p_cr/p_cr[-1] \n",
    "\n",
    "        # determine maximum in x and y direction\n",
    "        max_x = int(max([0, w, p_00[0], p_0r[0], p_c0[0], p_cr[0]]))\n",
    "        max_y = int(max([0, h, p_00[1], p_0r[1], p_c0[1], p_cr[1]]))\n",
    "\n",
    "        # determine minimum in x and y direction\n",
    "        offset_x = abs(int(min([0, w, p_00[0], p_0r[0], p_c0[0], p_cr[0]])))\n",
    "        offset_y = abs(int(min([0, h, p_00[1], p_0r[1], p_c0[1], p_cr[1]])))\n",
    "\n",
    "        # determine resulted size of warped image\n",
    "        dsize = (max_x + offset_x, max_y + offset_y)\n",
    "        print(\"image dsize =>\", dsize)\n",
    "\n",
    "        # add translation (offset) to inverse H \n",
    "        # => if warped corners are outside the image, we need translation\n",
    "        # H_inv_00, H_inv_01, H_inv_02 => tx=H_inv_02\n",
    "        # H_inv_10, H_inv_11, H_inv_12 => ty=H_inv_12\n",
    "        # H_inv_20, H_inv_21, H_inv_22\n",
    "        H_inv[0][-1] += offset_x # iH[0][-1]: last item of 1st row\n",
    "        H_inv[1][-1] += offset_y # iH[1][-1]: last item of 2nd row\n",
    "\n",
    "\n",
    "        # warp left image by inverse H and calulated size\n",
    "        output = cv2.warpPerspective(img_left, H_inv, dsize)\n",
    "        cv2.imshow(\"warped image\", output)\n",
    "        cv2.waitKey()\n",
    "\n",
    "        # add right image\n",
    "        output[offset_y:img_right.shape[0] + offset_y, offset_x:img_right.shape[1] + offset_x] = img_right\n",
    "\n",
    "        return output\n",
    "    else:\n",
    "        print(\"\\nError: not enough matches\\n\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stitch right image to left image (left is anchored image)\n",
    "def stitch_right(img_left, img_right, ratio, showMatches=True):\n",
    "\n",
    "    cv2.imshow(\"left image\", img_left)\n",
    "    cv2.imshow(\"right image\", img_right)\n",
    "    cv2.waitKey()\n",
    "\n",
    "    # get matches and keypoints of query image (left) in train image (right)\n",
    "    kp_left, kp_right, good = getKnnMatches(img_left, img_right, ratio, 'right')\n",
    "\n",
    "    # we need at least four matches to find homography between the images\n",
    "    if len(good) > 4:\n",
    "        \n",
    "        # extract locations (keypoints) of good matches\n",
    "        p_left, p_right = [], []\n",
    "        p_left = np.float32([kp_left[m[0].queryIdx].pt for m in good])\n",
    "        p_right = np.float32([kp_right[m[0].trainIdx].pt for m in good])\n",
    "        p_left = np.asarray(p_left)\n",
    "        p_right = np.asarray(p_right)\n",
    "        \n",
    "        # find homography using RANSAC (right image is src plane, left is target plane)\n",
    "        H, status = cv2.findHomography(p_right, p_left, cv2.RANSAC)        \n",
    "        \n",
    "        # calculate positions of warped corners of right image by matrix-vector product\n",
    "        #  p_00 ------------ p_c0\n",
    "        #   |                 |\n",
    "        #  p_0r ------------ p_cr\n",
    "        h = img_right.shape[0]\n",
    "        w = img_right.shape[1]\n",
    "        p_00 = np.dot(H, np.array([0,0,1])) \n",
    "        p_00 = p_00/p_00[-1] # homogenous to cartesian coordinates x=x/w, y=y/w\n",
    "        p_0r = np.dot(H, np.array([0, h, 1])) \n",
    "        p_0r = p_0r/p_0r[-1] \n",
    "        p_c0 = np.dot(H, np.array([w, 0, 1])) \n",
    "        p_c0 = p_c0/p_c0[-1] \n",
    "        p_cr = np.dot(H, np.array([w, h, 1])) \n",
    "        p_cr = p_cr/p_cr[-1] \n",
    "        # determine maximum in x and y direction\n",
    "        max_x = int(max([0, img_left.shape[1], p_00[0], p_0r[0], p_c0[0], p_cr[0]]))\n",
    "        max_y = int(max([0, img_left.shape[0], p_00[1], p_0r[1], p_c0[1], p_cr[1]]))\n",
    "        # determine minimum in x and y direction\n",
    "        offset_x = abs(int(min([0, img_left.shape[1], p_00[0], p_0r[0], p_c0[0], p_cr[0]])))\n",
    "        offset_y = abs(int(min([0, img_left.shape[0], p_00[1], p_0r[1], p_c0[1], p_cr[1]])))\n",
    "        # determine resulted size of warped image\n",
    "        dsize = (max_x + offset_x, max_y + offset_y)\n",
    "        print(\"image dsize =>\", dsize)\n",
    "        \n",
    "        # add translation (offset) to inverse H \n",
    "        # => if warped corners are outside the image, we need translation\n",
    "        # H_00, H_01, H_02 => tx=H_02\n",
    "        # H_10, H_11, H_12 => ty=H_12\n",
    "        # H_20, H_21, H_22\n",
    "        H[0][-1] += offset_x # iH[0][-1]: last item of 1st row\n",
    "        H[1][-1] += offset_y # iH[1][-1]: last item of 2nd row\n",
    "        \n",
    "        # perspective transformation of right image using computed homography\n",
    "        output = cv2.warpPerspective(img_right, H, dsize)\n",
    "        cv2.imshow(\"warped image\", output)\n",
    "        cv2.waitKey()\n",
    "        \n",
    "        # add left image\n",
    "        output[offset_y:img_left.shape[0] + offset_y, offset_x:img_left.shape[1] + offset_x] = img_left\n",
    "        \n",
    "        return output\n",
    "    else:\n",
    "        print(\"\\nError: not enough matches\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "img_left = cv2.imread('Image/01.jpg')\n",
    "img_right = cv2.imread('Image/02.jpg')\n",
    "path = 'Image/'\n",
    "\n",
    "# # stitch images\n",
    "# direction = 'right'\n",
    "# if direction=='right': # stitch images (from right to left)\n",
    "#     result = stitch_right(img_left, img_right, 0.6)\n",
    "# if direction=='left': # stitch images (from left to right)\n",
    "#     result = stitch_left(img_left, img_right, 0.7)\n",
    "\n",
    "# if result is not None:\n",
    "#     # write panorama\n",
    "#     cv2.imwrite(path +direction+\"_2_panorama.jpg\", result)\n",
    "\n",
    "#     # show panorama\n",
    "#     # resize image since cv2.imshow in python has problems to show big images\n",
    "#     scale = 70\n",
    "#     dim = int(result.shape[1] * scale / 100), int(result.shape[0] * scale / 100)\n",
    "#     pano = cv2.resize(result, dim) \n",
    "#     cv2.imshow(\"stitched images\", pano)\n",
    "#     cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/sudhir/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Homography:\n",
      " [[ 1.04461640e+00 -7.90827336e-02 -5.00873092e+02]\n",
      " [ 7.91160226e-02  9.73513755e-01 -1.19936026e+01]\n",
      " [ 2.40605319e-04 -1.21149686e-04  8.81908364e-01]]\n",
      "image dsize => (1294, 565)\n",
      "image dsize => (1711, 565)\n"
     ]
    }
   ],
   "source": [
    "# load and optionally resize images\n",
    "N = 3\n",
    "path = 'Image/'\n",
    "filename_prefix = '0'\n",
    "images = []\n",
    "\n",
    "# traversing over images through loop\n",
    "for i in range(N):\n",
    "    img = cv2.imread(path+filename_prefix+str(i+1)+'.jpg')\n",
    "    images.append(img)\n",
    "\n",
    "# iterate through the images to stitch by given direction\n",
    "direction = 'center'\n",
    "panorama = images[0]\n",
    "if direction=='left': \n",
    "    for i in range(1,N):\n",
    "        panorama = stitch_left(panorama, images[i], 0.7)\n",
    "\n",
    "if direction=='right': \n",
    "    for i in range(1,N):\n",
    "        panorama = stitch_right(panorama, images[i], 0.7)\n",
    "\n",
    "if direction=='center':\n",
    "    for i in range(1,N):\n",
    "        if i <= int(N/2) :\n",
    "            panorama = stitch_left(panorama, images[i], 0.7)\n",
    "        else:\n",
    "            panorama = stitch_right(panorama, images[i], 0.7)\n",
    "\n",
    "if panorama is not None:\n",
    "    # write panorama\n",
    "    cv2.imwrite(path+direction+\"_\"+str(i+1)+\"_panorama.jpg\", panorama)\n",
    "\n",
    "    # show panorama\n",
    "    # resize image since cv2.imshow in python has problems to show big images\n",
    "    scale = 70\n",
    "    dim = int(panorama.shape[1] * scale / 100), int(panorama.shape[0] * scale / 100)\n",
    "    panorama = cv2.resize(panorama, dim) \n",
    "    cv2.imshow(\"Panorama\", panorama)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitching using OpenCV\n",
    "#print(\"Enter the number of images:\")\n",
    "#nimg = int(input())\n",
    "\n",
    "# load and optionally resize images\n",
    "N = 3\n",
    "path = 'Image/'\n",
    "filename_prefix = '0'\n",
    "scale_percent = 80 # percentage of original size\n",
    "images = []\n",
    "\n",
    "# traversing over images through loop\n",
    "for i in range(N):\n",
    "    name = path+filename_prefix+str(i+1)+'.jpg'\n",
    "    # print(name)\n",
    "    img = cv2.imread(name)\n",
    "    # print(img.shape)\n",
    "    if scale_percent != 100:\n",
    "        dim = int(img.shape[1] * scale_percent / 100), int(img.shape[0] * scale_percent / 100)\n",
    "        img = cv2.resize(img, dim)\n",
    "\n",
    "    images.append(img)\n",
    "\n",
    "# initialize OpenCV image sticher object and then perform image stitching\n",
    "stitcher = cv2.Stitcher.create()\n",
    "(status, stitched) = stitcher.stitch(images)\n",
    "\n",
    "# if the status is '0', then OpenCV successfully performed image stitching\n",
    "if status == 0:\n",
    "\t# display images\n",
    "\tfor i in range(N):\n",
    "\t\tcv2.imshow(\"Input Image \"+str(i+1), images[i])\n",
    "  \n",
    "\tcv2.imshow(\"Stitched Image\", stitched)\n",
    "\tcv2.waitKey(0)\n",
    "\n",
    "# otherwise the stitching failed, e.g. not enough keypoints being detected\n",
    "else:\n",
    "\tprint(\"image stitching failed ({})\".format(status))\n",
    "\t\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Stabilization:\n",
    "\n",
    "Find Motion between frame\n",
    "Find good features in current and previous frame\n",
    "Find transformation by set of keypoints to map previous frame to current frame\n",
    "Store rotation and translation values\n",
    "Sum up values to create trajectory\n",
    "Smooth trajectory by average filter\n",
    "Calculate smooth motion between frame\n",
    "Apply smoothed camera motion to each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of frame: 797\n",
      "height: 480\n",
      "height: 272\n"
     ]
    }
   ],
   "source": [
    "def averageFiltering(path, ksize): \n",
    "    # define average filter \n",
    "    window_size = 2*ksize+1\n",
    "    filter = np.ones(window_size)/window_size \n",
    "\n",
    "    # add padding to the boundaries ('edge': repeat edge value)\n",
    "    path_pad = np.pad(path, (ksize, ksize), 'edge') \n",
    "\n",
    "    # apply convolution to given trajectory\n",
    "    path_smoothed = np.convolve(path_pad, filter, mode='same') \n",
    "\n",
    "    # return smoothed trajectory without padding\n",
    "    return path_smoothed[ksize:-ksize] \n",
    "\n",
    "# read video file\n",
    "path = 'Video/'\n",
    "cp = cv2.VideoCapture(path+'video.mp4')\n",
    "\n",
    "# get number of frames\n",
    "n_frames = int(cp.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(\"number of frame:\", n_frames)\n",
    "\n",
    "# get size of frame\n",
    "width = int(cp.get(cv2.CAP_PROP_FRAME_WIDTH)) \n",
    "height = int(cp.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(\"height:\", width)\n",
    "print(\"height:\", height)\n",
    "\n",
    "# read first frame and convert to grayscale\n",
    "_, prev = cp.read()\n",
    "prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# find camera motions between consequent frames: #\n",
    "##################################################\n",
    "\n",
    "# define array to hold translation (dx,dy) and rotation angle of each frame\n",
    "transforms = np.zeros((n_frames-1, 3), np.float32) \n",
    "\n",
    "for i in range(n_frames-2):\n",
    "    # find most prominent corners in previous frame by Shi-Tomasi detector\n",
    "    prev_pts = cv2.goodFeaturesToTrack(prev_gray, maxCorners=200, qualityLevel=0.01, minDistance=30, blockSize=3)\n",
    "\n",
    "    # read next frame\n",
    "    succ, frame = cp.read()\n",
    "\n",
    "    # convert frame to grayscale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # track feature points in current frame by optical flow\n",
    "    frame_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, frame_gray, prev_pts, None)\n",
    "\n",
    "    # estimate affine transformation between keypoints of previous and current frame\n",
    "    mat, inlier = cv2.estimateAffine2D(prev_pts, frame_pts) \n",
    "\n",
    "    # extract translation an rotation\n",
    "    dx = mat[0,2]\n",
    "    dy = mat[1,2]\n",
    "    angle = np.arctan2(mat[1,0], mat[0,0])\n",
    "    \n",
    "    # save translation and rotation angle\n",
    "    transforms[i] = [dx,dy,angle] \n",
    "    \n",
    "    # current frame will be the next previous frame\n",
    "    prev_gray = frame_gray\n",
    "\n",
    "# create trajectory by cumulative sum of transform for each dx,dy and angle\n",
    "trajectory = np.cumsum(transforms, axis=0) #axis=0: sum over rows for each of the 3 columns\n",
    "\n",
    "# smooth trajectory by applying average filter to x, y and angle \n",
    "smoothed_trajectory = np.copy(trajectory) \n",
    "ksize = 9\n",
    "for i in range(3):\n",
    "    smoothed_trajectory[:,i] = averageFiltering(trajectory[:,i], ksize)\n",
    "\n",
    "# take the difference to obtain smoothed trajectory\n",
    "difference = smoothed_trajectory - trajectory\n",
    "transforms_smooth = transforms + difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# apply smoothed camera motion to each frame: #\n",
    "###############################################\n",
    "# reset stream to first frame \n",
    "cp.set(cv2.CAP_PROP_POS_FRAMES, 0) \n",
    "for i in range(n_frames-2):\n",
    "\n",
    "    # read next frame\n",
    "    success, frame = cp.read() \n",
    "\n",
    "    # extract smoothed translations and angle\n",
    "    dx = transforms_smooth[i,0]\n",
    "    dy = transforms_smooth[i,1]\n",
    "    angle = transforms_smooth[i,2]\n",
    "\n",
    "    # set up transformation matrix by extracted values\n",
    "    m = np.zeros((2,3), np.float32)\n",
    "    m[0,0] = np.cos(angle)\n",
    "    m[0,1] = -np.sin(angle)\n",
    "    m[1,0] = np.sin(angle)\n",
    "    m[1,1] = np.cos(angle)\n",
    "    m[0,2] = dx\n",
    "    m[1,2] = dy\n",
    "\n",
    "    # apply affine warping to the given frame\n",
    "    frame_stabilized = cv2.warpAffine(frame, m, (width,height))\n",
    "\n",
    "    # concate before and after frame for comparing purpose\n",
    "    result = cv2.hconcat([frame, frame_stabilized])\n",
    "        \n",
    "    # show concated frames \n",
    "    cv2.imshow(\"Video Stabilization\", result)\n",
    "    cv2.waitKey(10)\n",
    "\n",
    "cp.release() # release video\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
