{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harris Corner Detector:\n",
    "\n",
    "- Apply Gaussian window function to avoid noisy response\n",
    "- considers small shifts by first order taylor series expansion\n",
    "- Determine intensity change by analyzing the eigenvalues of second-moment matrix\n",
    "\n",
    "- criteria for Harris response\n",
    "    - R_xy = det(M) - K * trace (M)^2 > threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harrisCorners(image, ksize, windowSize, k, thresholdRatio):\n",
    "\n",
    "    # get dimension of input image (image : grayscale image)\n",
    "    height, width = image.shape\n",
    "\n",
    "    # define output matrix for harris response\n",
    "    harris_response = np.zeros((height, width), np.float32)\n",
    "\n",
    "    # define array for indicies of thresholded harris response\n",
    "    corners = []\n",
    "\n",
    "    # compute image gradient dx, and dy : floating point precision is needed => set ddepth of output to float\n",
    "    dx = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize)\n",
    "    dy = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize)\n",
    "\n",
    "    # Compute squared gradient dxx, dyy and dxy\n",
    "    dxx = dx * dx\n",
    "    dyy = dy * dy\n",
    "    dxy = dx * dy\n",
    "\n",
    "    # Apply gaussian window function to squared gradients, here we GaussianBlur()\n",
    "    dx2 = cv2.GaussianBlur(dxx, (3,3),0)\n",
    "    dy2 = cv2.GaussianBlur(dyy, (3,3),0)\n",
    "    dxy2 = cv2.GaussianBlur(dxy, (3,3),0)\n",
    "\n",
    "    # Compute the sums of the squared gradient at each pixel p(y,x)\n",
    "    offset = int(windowSize/2)\n",
    "    for y in range(offset, height-offset): \n",
    "        for x in range(offset, width-offset):\n",
    "\n",
    "            # calculate sum of squared gradients\n",
    "            Sx2 = np.sum(dx2[y-offset:y+offset, x-offset:x+offset])\n",
    "            Sy2 = np.sum(dy2[y-offset:y+offset, x-offset:x+offset])\n",
    "            Sxy = np.sum(dxy2[y-offset:y+offset, x-offset:x+offset])\n",
    "\n",
    "            # define the second moment matrix \n",
    "            M = np.array([[Sx2, Sxy], [Sxy, Sy2]])\n",
    "\n",
    "            # calculate the response function \n",
    "            det = np.linalg.det(M) # det = Sx2*Sy2 - Sxy**2\n",
    "            trace = np.matrix.trace(M) # trace = Sx2 + Sy2\n",
    "            harris_response[y, x] = det - k*(trace**2) # Harris response function\n",
    "\n",
    "    # show response matrix\n",
    "    # cv2.imshow(\"Response Matrix\", harris_response)\n",
    "\n",
    "    # determine threshold using maximum corner response\n",
    "    threshold = thresholdRatio * harris_response.max()\n",
    "\n",
    "    # Apply threshold and get corners\n",
    "    corners = np.argwhere(harris_response > threshold)\n",
    "\n",
    "     # binarize response matrix by threshold for display purpose\n",
    "    harris_response[harris_response <= threshold] = 0\n",
    "    harris_response[harris_response > threshold] = 255\n",
    "    # cv2.imshow(\"Response matrix\", harris_response)\n",
    "\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "    return corners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "img = cv2.imread('Image/house1.jpg',1)\n",
    "# plt.imshow(img)\n",
    "\n",
    "\n",
    "img_copencv = img.copy()\n",
    "\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# define kernel\n",
    "KSobel_Size = 3\n",
    "window_Size = 3\n",
    "\n",
    "# define harris free parameters k and threshold ratio\n",
    "k =0.06  # 0.04 - 0.06\n",
    "threshold_ratio = 0.01\n",
    "\n",
    "# find corners by harris detector\n",
    "harris_Corners = harrisCorners(img_gray, KSobel_Size, window_Size, k, threshold_ratio)\n",
    "\n",
    "# highlight harris corner in input image\n",
    "for x,y in harris_Corners:\n",
    "    # img[x,y] = [0,0,255]\n",
    "    cv2.circle(img, (y,x), 3, (0,0,255))\n",
    "\n",
    "# show the images\n",
    "cv2.imshow(\"Harris Detector\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Harris corner detector from OpenCV\n",
    "\n",
    "corners = cv2.cornerHarris(img_gray, window_Size-1, KSobel_Size, k)\n",
    "\n",
    "index = np.argwhere(corners > threshold_ratio * corners.max())\n",
    "\n",
    "for x,y in index:\n",
    "    cv2.circle(img_copencv, (y,x), 3, (0,0,255))\n",
    "\n",
    "# binarize OpenCV Harris corners for display purpose\n",
    "corners[corners <= threshold_ratio*corners.max()] = 0\n",
    "corners[corners > threshold_ratio*corners.max()] = 255\n",
    "\n",
    "\n",
    "cv2.imshow(\"OpneCV Harris corner detector\", img_copencv)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits for Harris corner detectors:\n",
    "- invariant to image rotation\n",
    "- partially invariant to affine intensity changes\n",
    "- not invariant to scaling\n",
    "\n",
    "Feature Description\n",
    "\n",
    "- Feature Detector:\n",
    "\n",
    "    • calculates locations of significant areas in an image (e.g., corners)\n",
    "    \n",
    "    • no other information about the detected features\n",
    "\n",
    "- Feature Descriptor:\n",
    "\n",
    "    • local description of surrounding pixels\n",
    "\n",
    "    • outputs feature vectors\n",
    "\n",
    "    • numerical \"fingerprint\"\n",
    "\n",
    "    • ideally: invariant under image transformation\n",
    "    \n",
    "    • descriptor vector for each feature point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Matching:\n",
    "\n",
    "• Descriptors are compared across images to identify similar features\n",
    "\n",
    "• Useful to solve N-view correspondence problems\n",
    "\n",
    "Process of Feature Matching consists of:\n",
    "1. Find interest points (feature detection)\n",
    "2. Compute descriptor vector for each feature point (feature description)\n",
    "3. Compare descriptors across images to identify similar pixels (feature matching)\n",
    "\n",
    "Invariances of Harris Corner Descriptor:\n",
    "\n",
    "    • not invariant to rotation\n",
    "\n",
    "    • not invariant to scaling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT - scale invariant feature transform\n",
    "\n",
    "Characteristics of SIFT:\n",
    "\n",
    "• detecting keypoints in scale space\n",
    "\n",
    "• assigning an orientation to each keypoint\n",
    "\n",
    "• descriptor to each keypoint (16x16 neighbourhood around keypoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load image\n",
    "\n",
    "image1 = cv2.imread('Image/house1.jpg')\n",
    "im1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "image2 = cv2.imread('Image/house2.jpg')\n",
    "im2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# construct a SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "\n",
    "# detect SIFT keypoints and compute descriptors from keypoints\n",
    "# kp1, des1 = sift.detectAndCompute(im1, None)\n",
    "# kp2, des2 = sift.detectAndCompute(im2, None)\n",
    "\n",
    "# also we can detect kp, and compute descriptors separetly for ORB\n",
    "kp1 = orb.detect(im1)\n",
    "kp1, des1 = orb.compute(im1, kp1)\n",
    "kp2 = orb.detect(im2)\n",
    "kp2, des2 = orb.compute(im2, kp2)\n",
    "\n",
    "\n",
    "# detect \n",
    "\n",
    "# Brute force matcher\n",
    "matcher = cv2.BFMatcher()\n",
    "matches = matcher.match(des1, des2)\n",
    "\n",
    "# sort matches in the order of their distance\n",
    "matches = sorted(matches, key=lambda x:x.distance)\n",
    "\n",
    "# draw first 100 matches\n",
    "cv2.drawKeypoints(image1, kp1, image1)\n",
    "cv2.drawKeypoints(image2, kp2, image2)\n",
    "matched_image = cv2.drawMatches(image1, kp1, image2, kp2, matches[:10], 0)\n",
    "\n",
    "cv2.imshow(\"Feature matching\", matched_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SURF (Speeded-Up Robust Features):\n",
    "\n",
    "    • fast SIFT\n",
    "\n",
    "    • approximates Laplacian of Gaussian with Box Filters\n",
    "    \n",
    "    • uses wavelets responses for orientation assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load image\n",
    "# image1 = cv2.imread('Image/house1.jpg')\n",
    "# img_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# # create SURF object\n",
    "# surf = cv2.SURF()\n",
    "\n",
    "# # detect keypoints and compute descriptors with SURF\n",
    "# kp, des = surf.detectAndCompute(img_gray, None)\n",
    "\n",
    "# # draw keypoints on top of the input image\n",
    "# cv2.drawKeypoints(img, kp, img)\n",
    "# cv2.imshow(\"SURF\", img)\n",
    "\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAST (Features from Accelerated Segment Test):\n",
    "• for real-time application (e.g., SLAM for mobile robots)\n",
    "• is a feature detector, not a descriptor extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # load image\n",
    "# image1 = cv2.imread('Image/house1.jpg')\n",
    "# img_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # create FAST object for feature detection\n",
    "# fast = cv2.FastFeatureDetector()\n",
    "\n",
    "# # detect keypoints with FAST\n",
    "# kp = fast.detect(img_gray, None)\n",
    "\n",
    "# # create BRISK object for feature description\n",
    "# # BRISK: Binary Robust Invariant Scalable Keypoints\n",
    "# br = cv2.BRISK_create()\n",
    "\n",
    "# # compute the descriptors with BRISK\n",
    "# kp, des = br.compute(img_gray, kp)\n",
    "\n",
    "# cv2.imshow(kp)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "- SIFT (Scale Invariant Feature Transform)\n",
    "\n",
    "- SURF (Speeded-Up Robust Features)\n",
    "\n",
    "- FAST (Features from Accelerated Segment Test)\n",
    "\n",
    "- BRIEF (Binary Robust Independent Elementary features)\n",
    "\n",
    "- ORB (Oriented FAST and Rotated BRIEF):\n",
    "\n",
    "    • combines FAST keypoint detector and BRIEF descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
